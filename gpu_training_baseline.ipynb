{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os, json, cv2\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\n\n#model imports (keras/tensorflow)\nimport tensorflow as tf\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.preprocessing import image\nfrom keras import layers, models\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom keras.optimizers import Adam\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization, GlobalAveragePooling2D\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping, ReduceLROnPlateau","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Notes\n\n- NORMALIZE THE LAYERS IT TAKES WAY TOO LONG TO TRAIN\n- Best results 512 img_size, might want to use 224 to save GPU if toying with model\n- Make Normalization Layers Non-Trainable for pretrained model\n    - https://www.kaggle.com/bibhash123/cassava-classification-simple-overview-tf-keras\n- (custom loss func) - Bi-Tempered Logistic Loss or other?\n- (cross-validation strat) - 5 fold cross validation"},{"metadata":{},"cell_type":"markdown","source":"Notebooks that helped me, and taught me some awesome things along the way.\n\n[Maksym Shkliarevskyis Notebook](https://www.kaggle.com/maksymshkliarevskyi/cassava-leaf-disease-best-keras-cnn)\n\n[Chris Deotte's GPU/TPU Augmentation Notebook](https://www.kaggle.com/cdeotte/rotation-augmentation-gpu-tpu-0-96)"},{"metadata":{},"cell_type":"markdown","source":"# Load Data"},{"metadata":{"trusted":true},"cell_type":"code","source":"WORK_DIR = '../input/cassava-leaf-disease-classification'\n\n#counting number of images in the train_images folder\nprint('Train images: %d' %len(os.listdir(os.path.join(WORK_DIR, \"train_images\"))))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading labels from .json file\nwith open(os.path.join(WORK_DIR, \"label_num_to_disease_map.json\")) as file:\n    print(json.dumps(json.loads(file.read()), indent=4))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"The labels for each image are stored in a csv file. We can read these in to a pandas dataframe and view the top five rows of this dataFrame."},{"metadata":{"trusted":true},"cell_type":"code","source":"#reading in the training image labels \nall_train_labels_df = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\nall_train_labels_df.head(3)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualizing Data"},{"metadata":{},"cell_type":"markdown","source":"Although I find it is quite difficult for the human eye to determine any major differnces between the leaves in this dataset, I feel it is important to plot a few images to get a feel the nature of the dataset. \n\n- I used .sample() to take three random images from each group. This is handy as everytime the notebook is ran we see a different set of images."},{"metadata":{},"cell_type":"markdown","source":"###  \"0\" : Cassava Bacterial Blight (CBB)"},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1\nplt.figure(figsize=(40, 40))\nfor image_id in all_train_labels_df.loc[all_train_labels_df.label==0].sample(3).image_id.values:\n    img = cv2.imread(os.path.join(WORK_DIR, \"train_images\", image_id)) #using cv2 library to read imgs\n    plt.subplot(1, 3, i)\n    plt.imshow(img)\n    plt.title(image_id, fontsize=30)\n    i+=1\n\nplt.show()\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"###  \"1\" : Cassava Brown Streak Disease (CBSD)"},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1\nplt.figure(figsize=(40, 40))\nfor image_id in all_train_labels_df.loc[all_train_labels_df.label==1].sample(3).image_id.values:\n    img = cv2.imread(os.path.join(WORK_DIR, \"train_images\", image_id)) #using cv2 library to read imgs\n    plt.subplot(1, 3, i)\n    plt.imshow(img)\n    plt.title(image_id, fontsize=30)\n    i+=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### \"2\" : Cassava Green Mottle (CGM)"},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1\nplt.figure(figsize=(40, 40))\nfor image_id in all_train_labels_df.loc[all_train_labels_df.label==2].sample(3).image_id.values:\n    img = cv2.imread(os.path.join(WORK_DIR, \"train_images\", image_id)) #using cv2 library to read imgs\n    plt.subplot(1, 3, i)\n    plt.imshow(img)\n    plt.title(image_id, fontsize=30)\n    i+=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### \"3\" : Cassava Mosaic Disease (CMD)"},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1\nplt.figure(figsize=(40, 40))\nfor image_id in all_train_labels_df.loc[all_train_labels_df.label==3].sample(3).image_id.values:\n    img = cv2.imread(os.path.join(WORK_DIR, \"train_images\", image_id)) #using cv2 library to read imgs\n    plt.subplot(1, 3, i)\n    plt.imshow(img)\n    plt.title(image_id, fontsize=30)\n    i+=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### \"4\" : Healthy"},{"metadata":{"trusted":true},"cell_type":"code","source":"i=1\nplt.figure(figsize=(40, 40))\nfor image_id in all_train_labels_df.loc[all_train_labels_df.label==4].sample(3).image_id.values:\n    img = cv2.imread(os.path.join(WORK_DIR, \"train_images\", image_id)) #using cv2 library to read imgs\n    plt.subplot(1, 3, i)\n    plt.imshow(img)\n    plt.title(image_id, fontsize=30)\n    i+=1\n\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data Augmentation + Variables\n\nSTEPS_PER_EPOCH - great parameter to user when making augmented data on the fly. This is basically the number of batch iterations before the epoch is considered finished.\n\nVALIDATION_STEPS - Same as STEPS_PER_EPOCH but is used when we are testing the model on the validation dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"#model parameters values used to aid in readability and debugging etc.\nBATCH_SIZE = 16\nEPOCHS = 15\nTARGET_SIZE = 224 #parameter that specifies the image dimensions '224 for test, 512 for highest score'\nSTEPS_PER_EPOCH = len(all_train_labels_df)*0.9 / BATCH_SIZE #number of batch iterations before epoch is considered done\nVALIDATION_STEPS = len(all_train_labels_df)*0.1 / BATCH_SIZE","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"I would like to do a stratified test split on the data so that I have both a training and validation dataset that is representative of the entire dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#setting train_labels to string before train_test_split\nall_train_labels_df.label = all_train_labels_df.label.astype('str')\n\n#stratified train_test_split\ntrain_labels_df, valid_labels_df = train_test_split(all_train_labels_df, random_state=2718,\n                                      test_size=0.10, stratify = all_train_labels_df.label)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Just making sure we are getting an even distribution of labels across the train and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_labels_df.label.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"valid_labels_df.label.value_counts(normalize=True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Using very intuituive keras image preprocessing framework called ImageDataGenerator.\n\n- documentation and parameter guide found here -> [link](https://keras.io/api/preprocessing/image/#imagedatagenerator-class)"},{"metadata":{"trusted":true},"cell_type":"code","source":"my_data_augmentor = ImageDataGenerator(preprocessing_function = None,\n                    rotation_range = 45,\n                    zoom_range = 0.2,\n                    horizontal_flip = True,\n                    vertical_flip = True,\n                    fill_mode = 'nearest', #set to default val\n                    shear_range = 0.1, #shear_range is distortion along axis, which changes the angle of the image\n                    height_shift_range = 0.1,\n                    width_shift_range = 0.1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"This is how we read through different images in the work directory and apply the imagedatagenerator as they are read in.\n\nIt is also important to note that the typical train_test_split of the data is occuring in the imagedatagenerator. We "},{"metadata":{"trusted":true},"cell_type":"code","source":"training_generator = my_data_augmentor.flow_from_dataframe(train_labels_df,\n                         directory = os.path.join(WORK_DIR, \"train_images\"),\n                         x_col = \"image_id\",\n                         y_col = \"label\",\n                         target_size = (TARGET_SIZE, TARGET_SIZE),\n                         batch_size = BATCH_SIZE,\n                         class_mode = \"sparse\")\n\nvalidation_generator = my_data_augmentor.flow_from_dataframe(valid_labels_df,\n                         directory = os.path.join(WORK_DIR, \"train_images\"),\n                         x_col = \"image_id\",\n                         y_col = \"label\",\n                         target_size = (TARGET_SIZE, TARGET_SIZE),\n                         batch_size = BATCH_SIZE,\n                         class_mode = \"sparse\") #Determines the type of label arrays that are returned","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing Augmented Data\n\nThe following sub section is pretty cool, it enables me to alter the parameters of the ImageDataGnerator, and visualize how this affects an image before committing those parameters to a model to train."},{"metadata":{"trusted":true},"cell_type":"code","source":"generator = my_data_augmentor.flow_from_dataframe(train_labels_df.iloc[153:154],\n                         directory = os.path.join(WORK_DIR, \"train_images\"),\n                         x_col = \"image_id\",\n                         y_col = \"label\",\n                         target_size = (TARGET_SIZE, TARGET_SIZE),\n                         batch_size = BATCH_SIZE,\n                         class_mode = \"categorical\")\n\naug_images = [generator[0][0][0]/255 for i in range(10)]\nfig, axes = plt.subplots(2, 5, figsize = (20, 10))\naxes = axes.flatten()\nfor img, ax in zip(aug_images, axes):\n    ax.imshow(img)\n    ax.axis('off')\nplt.tight_layout()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Creating the model\n\nTo start of I am going to be building off one of tensorflows pretrained models called efficientnetB0. I will be trying out models using different base models ie VGG, Inception, and ResNet. Im not entirely sure which model is best for this situation, but thats what the internet is for. I am looking into this topic!\n\nThe following cell builds and saves the model below. This has previously been saved to a dataset and we can loads this back in with its optimal weights.\n\nNote we are using \"sparse_categorical_crossentropy\" because we did not one hot encode our training labels. If we did one_hot encode labels we would use \"categorical_crossentropy\""},{"metadata":{"trusted":true},"cell_type":"code","source":"# !git clone https://github.com/Diulhio/bitemperedloss-tf.git\n# import sys\n# sys.path.append('./bitemperedloss-tf')\n# from tf_bi_tempered_loss import BiTemperedLogisticLoss\n\n# # loss=BiTemperedLogisticLoss(t1=1.0, t2=1.0, label_smoothing=0.1)\n# loss=BiTemperedLogisticLoss(t1=1.0, t2=1.0)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#make sure internet is enabled in notebook so we can access efficientNetB0 model from google.storage.api\ndef create_model():\n    model = models.Sequential()\n\n    model.add(EfficientNetB0(include_top = False, weights = 'imagenet', \n                             input_shape = (TARGET_SIZE, TARGET_SIZE, 3)))\n    \n    model.add(layers.GlobalAveragePooling2D())\n    model.add(layers.Dense(5, activation = \"softmax\"))# 5 is the dimensionality of the output space \"5 options\"\n\n    model.compile(optimizer = 'adam',\n                  loss = \"sparse_categorical_crossentropy\",\n                  metrics = [\"acc\"])\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = create_model()\nprint('CNN has {} layers'.format(len(model.layers)))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('./EfNetB0_untrained_progress_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Callbacks"},{"metadata":{},"cell_type":"markdown","source":"ModelCheckpoint docs - https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/ModelCheckpoint\n\nReduceLROnPlateau docs - https://keras.io/api/callbacks/reduce_lr_on_plateau/"},{"metadata":{"trusted":true},"cell_type":"code","source":"#callbacks\nmodel_save = ModelCheckpoint('./EffNetB0_best_progress_weights_model.h5', \n                             save_best_only = True, \n                             save_weights_only = True,\n                             monitor = 'val_loss', \n                             mode = 'min',\n                             save_freq='epoch',\n                             verbose = 1)\n\nmy_early_stopper = EarlyStopping(monitor = 'val_loss', min_delta = 0.001, \n                           patience = 5, mode = 'min', verbose = 1,\n                           restore_best_weights = True)\n\n# reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.3, \n#                               patience = 2, min_delta = 0.001, \n#                               mode = 'min', verbose = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Custom LR Scheduler\n\nThe warm-up phase is important so that we do not have any catsrtophic forgetting of model weights happening at the beginning of training on the pretrained model.\n\nA custom LR scheduler is common practice for transfer learning. The learning rate starts near zero, then increases to a maximum, then decays over time.\n\n[Chris Deotte's GPU/TPU Aug](https://www.kaggle.com/cdeotte/rotation-augmentation-gpu-tpu-0-96)"},{"metadata":{"trusted":true},"cell_type":"code","source":"# Learning rate schedule for TPU, GPU and CPU.\n# Using an LR ramp up because fine-tuning a pre-trained model.\n# Starting with a high LR would break the pre-trained weights.\n\nLR_START = 0.0005\nLR_MAX = 0.001\nLR_MIN = 0.0005\nLR_RAMPUP_EPOCHS = 7\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) / LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \n#setting verbose=True allows us to see LR in model training\nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose = True)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Visualizing the Custom LR Scheduler"},{"metadata":{"trusted":true},"cell_type":"code","source":"rng = [i for i in range(25 if EPOCHS<25 else EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Fit Model"},{"metadata":{"trusted":true},"cell_type":"code","source":" history = model.fit(\n     training_generator,\n     steps_per_epoch = STEPS_PER_EPOCH,\n     epochs = EPOCHS,\n     validation_data = validation_generator,\n     validation_steps = VALIDATION_STEPS,\n     callbacks = [model_save, my_early_stopper, lr_callback]\n )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('./EffNetB0_trained_progress_model.h5')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Visualizing Model History"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13, 5))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Test'])\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.figure(figsize=(13, 5))\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('Model Accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['Train','Test'])\nplt.grid()\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Prediction\n\nMaking predictions in a seperate notebook, just keeping code here as a guide."},{"metadata":{"trusted":true},"cell_type":"code","source":"# sample_submission = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\n# sample_submission","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#making preds on all the test_images\n# preds = []\n\n# for image_id in sample_submission.image_id:\n#     image = Image.open(os.path.join(WORK_DIR,  \"test_images\", image_id))\n#     image = image.resize((TARGET_SIZE, TARGET_SIZE))\n#     image = np.expand_dims(image, axis = 0)\n#     preds.append(np.argmax(model.predict(image)))\n\n# sample_submission['label'] = preds\n# sample_submission","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}